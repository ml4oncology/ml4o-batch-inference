# ml4o-batch-inference
Scalable batch inference pipeline using vLLM. Process thousands of prompts on HPC clusters with Docker/Apptainer and Slurm support.
